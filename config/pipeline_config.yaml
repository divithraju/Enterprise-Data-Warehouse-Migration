hadoop_user: "divithraju"
name_node: "hdfs://localhost:4444"
hdfs_base: "/user/divithraju/hr_project"
raw_path: "/user/divithraju/hr_project/raw"
processed_path: "/user/divithraju/hr_project/processed"
archive_path: "/user/divithraju/hr_project/archive"
logs_path: "/user/divithraju/hr_project/logs"
validation_path: "/user/divithraju/hr_project/validation"

mysql:
  host: "localhost"
  port: 3306
  database: "retail_sales"
  user: "divithraju"
  password: "divith4321"
  table: "sales_transactions"

spark:
  app_name: "Sales_ETL_Pipeline"
  master: "local[*]"
  executor_memory: "2g"
  driver_memory: "1g"

hive:
  database: "sales_warehouse"
  table: "sales_summary"

scheduler:
  type: "airflow"
  frequency: "weekly"

validation:
  enable_data_quality_checks: true
  null_threshold: 0.05
  duplicate_threshold: 0.02

logging:
  level: "INFO"
  log_file: "/user/divithraju/hr_project/logs/pipeline.log"
